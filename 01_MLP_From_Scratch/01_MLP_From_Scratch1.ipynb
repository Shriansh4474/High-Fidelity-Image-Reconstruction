{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvoRof3bX0ok"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wandb'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "def add_username_watermark (username = \"shriansh.sahu\"):\n",
        "\n",
        "    plt.text(\n",
        "        0.98,0.02,username,\n",
        "        ha = 'right' , va = 'bottom',\n",
        "        transform = plt.gca().transAxes,\n",
        "        fontsize = 10 , color = 'gray' , alpha = 0.7\n",
        "    )\n",
        "\n",
        "\n",
        "class Identity:\n",
        "    def forward(self,z) : return z\n",
        "    def backward(self,z): return np.ones(z.shape)\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self,z) : return 1/(1 + np.exp(-np.clip(z, -500 , 500)))\n",
        "    def backward(self,z):\n",
        "        s = self.forward(z)\n",
        "        return s * (1-s)\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self,z) : return np.tanh(z)\n",
        "    def backward(self,z): return 1 - np.tanh(z) ** 2\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self,z) : return np.maximum(0,z)\n",
        "    def backward(self,z): return (z >0).astype(float)\n",
        "\n",
        "class MSE:\n",
        "    def forward(self, y_true , y_pred):\n",
        "        return np.mean((y_pred - y_true)**2)\n",
        "\n",
        "    def backward(self, y_true , y_pred):\n",
        "        return 2*(y_pred - y_true) / y_true.size\n",
        "\n",
        "class BCE:\n",
        "    def forward(self, y_true , y_pred):\n",
        "        epsilon = 1e-5\n",
        "        y_pred = np.clip(y_pred , epsilon , 1-epsilon)\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def backward(self, y_true , y_pred):\n",
        "       epsilon = 1e-5\n",
        "       y_pred = np.clip(y_pred , epsilon , 1-epsilon)\n",
        "       return ((1 - y_true) / ( 1 - y_pred) - y_true / y_pred) / y_true.size\n",
        "\n",
        "\n",
        "class Linear:\n",
        "\n",
        "    def __init__(self ,input_width  , output_width , activation ):\n",
        "        limit = np.sqrt(6 / (input_width + output_width))\n",
        "        self.weights = np.random.uniform(-limit , limit, ( input_width, output_width))\n",
        "        self.biases = np.zeros((1 , output_width))\n",
        "        self.activation = activation()\n",
        "        self.input , self.z , self.a = None, None, None\n",
        "        self.grad_weights_cumulative = np.zeros_like(self.weights)\n",
        "        self.grad_biases_cumulative = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input , self.z = input_data , np.dot(input_data , self.weights) + self.biases\n",
        "        self.a = self.activation.forward(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward( self , upstream_gradient):\n",
        "        d_z = upstream_gradient * self.activation.backward(self.z)\n",
        "        self.grad_weights_cumulative += np.dot(self.input.T , d_z)\n",
        "        self.grad_biases_cumulative += np.sum(d_z , axis = 0 , keepdims= True)\n",
        "        return np.dot(d_z , self.weights.T)\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, layers , loss_function):\n",
        "        self.layers , self.loss_fn = layers, BCE() if loss_function.lower() == 'bce' else MSE()\n",
        "\n",
        "    def forward(self, x):\n",
        "            for layer in self.layers : x = layer.forward(x)\n",
        "            return x\n",
        "\n",
        "    def backward(self, y_true , y_pred):\n",
        "            grad = self.loss_fn.backward(y_true , y_pred)\n",
        "            for layer in reversed(self.layers): grad = layer.backward(grad)\n",
        "\n",
        "    def train(self,x,y):\n",
        "            y_pred = self.forward(x)\n",
        "            loss = self.loss_fn.forward(y , y_pred)\n",
        "            self.backward(y , y_pred)\n",
        "            return loss , y_pred\n",
        "\n",
        "    def calculate_loss(self , x , y):\n",
        "            y_pred = self.forward(x)\n",
        "            loss = self.loss_fn.forward(y , y_pred)\n",
        "            return loss\n",
        "\n",
        "    def zero_grad(self):\n",
        "            for layer in self.layers:\n",
        "                layer.grad_weights_cumulative.fill(0)\n",
        "                layer.grad_biases_cumulative.fill(0)\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "            for layer in self.layers:\n",
        "                layer.weights -= learning_rate * layer.grad_weights_cumulative\n",
        "                layer.biases -= learning_rate * layer.grad_biases_cumulative\n",
        "\n",
        "    def predict(self,x):\n",
        "            return self.forward(x)\n",
        "\n",
        "\n",
        "class XORProblem:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.X = np.array([[0,0] , [0,1] , [1,0] , [1,1]])\n",
        "        self.y = np.array([[0] ,[1] ,[1] ,[0]])\n",
        "\n",
        "    def run_test(self ,model_layers, epochs = 5000 , lr = 0.1):\n",
        "\n",
        "        model = Model(layers=model_layers , loss_function='bce')\n",
        "        for epoch in range(epochs):\n",
        "            model.zero_grad()\n",
        "            loss, _ = model.train(self.X, self.y)\n",
        "            model.update(lr)\n",
        "\n",
        "            if(epoch+1) % 1000 == 0:\n",
        "                print(f\" Epoch {epoch+1} / {epochs}, Loss: {loss: .6f}\")\n",
        "\n",
        "        predictions = model.predict(self.X)\n",
        "        binary_preds = (predictions > 0.5).astype(int)\n",
        "        accuracy = np.mean(binary_preds == self.y) * 100\n",
        "        print(f\" Final Accuracy:{accuracy: .2f}%\")\n",
        "        if accuracy == 100.0 :\n",
        "            print(\" Status : Converged Successfully\")\n",
        "        else:\n",
        "            print(\"Status : Failed to converged\")\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class GradientChecker:\n",
        "\n",
        "    def check(self, model , x ,y, epsilon = 1e-5 , debug = False):\n",
        "        print(\"\\n Runnig Gradient Check\")\n",
        "\n",
        "        model.zero_grad()\n",
        "        y_pred = model.forward(x)\n",
        "        loss = model.loss_fn.forward(y , y_pred)\n",
        "        model.backward(y ,y_pred)\n",
        "\n",
        "        analytical_list = []\n",
        "        for layer in model.layers:\n",
        "            analytical_list.append(layer.grad_weights_cumulative.ravel())\n",
        "            analytical_list.append(layer.grad_biases_cumulative.ravel())\n",
        "        analytical_grads_flat = np.concatenate(analytical_list)\n",
        "\n",
        "        numerical_grads = []\n",
        "        for layer in model.layers:\n",
        "            for param_tensor in [layer.weights , layer.biases]:\n",
        "                it = np.nditer(param_tensor , flags=['multi_index'],op_flags=['readwrite'])\n",
        "                while not it.finished:\n",
        "                    idx = it.multi_index\n",
        "                    orig = param_tensor[idx]\n",
        "\n",
        "                    param_tensor[idx] = orig + epsilon\n",
        "                    loss_plus = model.calculate_loss(x,y)\n",
        "\n",
        "                    param_tensor[idx] = orig - epsilon\n",
        "                    loss_minus = model.calculate_loss(x,y)\n",
        "\n",
        "                    numerical_grads.append((loss_plus - loss_minus) / (2 * epsilon))\n",
        "\n",
        "                    param_tensor[idx] = orig\n",
        "                    it.iternext()\n",
        "\n",
        "        numerical_grads_flat = np.array(numerical_grads)\n",
        "\n",
        "        if analytical_grads_flat.shape != numerical_grads_flat.shape:\n",
        "            raise RuntimeError(\"Shape Mismatch of analytical and numerical grad\")\n",
        "\n",
        "        numerator = np.linalg.norm(analytical_grads_flat - numerical_grads_flat)\n",
        "        denominator = np.linalg.norm(analytical_grads_flat) + np.linalg.norm(numerical_grads_flat)\n",
        "        relative_error = numerator / (denominator + 1e-12)\n",
        "\n",
        "        print(f\"Shape of Analytical Gradients : {analytical_grads_flat.shape}\")\n",
        "        print(f\" Shape of Numerical Gradient : {numerical_grads_flat.shape}\")\n",
        "        print(f\"Relative Error : {relative_error: .2e}\")\n",
        "\n",
        "        if relative_error < 1e-7:\n",
        "            print(\" Gradient Check Passed\")\n",
        "\n",
        "        else:\n",
        "            print(\"Gradient Check failed\")\n",
        "\n",
        "\n",
        "\n",
        "class ImageBorderDataset:\n",
        "\n",
        "    def __init__(self, image_path):\n",
        "        self.pixels,self.width ,self.height = self._load_and_process_image(image_path)\n",
        "\n",
        "    def _load_and_process_image(self , image_path):\n",
        "        try: image = Image.open(image_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            arr = np.zeros((50,50,3), dtype=np.uint8)\n",
        "            arr[:25,:,:] = [255 , 165 ,0]\n",
        "            arr[25:,:,:] = [138 ,43 ,226]\n",
        "            image = Image.fromarray(arr)\n",
        "\n",
        "        img_array = np.array(image)\n",
        "        height ,width,_ = img_array.shape\n",
        "        data = [(((x / (width-1) ,y / (height -1))) , 1 if all(img_array[y,x][:2] < [150,50]) and img_array[y,x][2] > 100 else 0) for y in range(height) for x in range(width)]\n",
        "        return data, width , height\n",
        "\n",
        "    def get_shuffled_data(self):\n",
        "        shuffled  = self.pixels[:]\n",
        "        random.shuffle(shuffled)\n",
        "        return shuffled\n",
        "\n",
        "\n",
        "def evaluate_and_visualize(model, dataset ,run_folder):\n",
        "\n",
        "    ORANGE , PURPLE ,RED = [255,165,0],[138,43,226],[255,0,0]\n",
        "    coords = np.array([p[0] for p in dataset.pixels])\n",
        "    true_labels = np.array([p[1] for p in dataset.pixels])\n",
        "    pred_labels = (model.predict(coords) > 0.5).astype(int).flatten()\n",
        "    accuracy  = np.mean(pred_labels == true_labels) * 100\n",
        "    print(f\"Final Pixel Accuracy: {accuracy: .2f}%\")\n",
        "    wandb.log({\"final_accuracy\" : accuracy})\n",
        "\n",
        "    true_map , pred_map = true_labels.reshape(dataset.height , dataset.width), pred_labels.reshape(dataset.height , dataset.width)\n",
        "    def create_color_map(m):\n",
        "        c = np.zeros((dataset.height , dataset.width ,3) , dtype = np.uint8)\n",
        "        c[m == 0] ,c[m == 1] = ORANGE , PURPLE\n",
        "        return c\n",
        "    gt_img , pred_img = create_color_map(true_map),create_color_map(pred_map)\n",
        "    err_img = gt_img.copy()\n",
        "    err_img [true_map != pred_map] = RED\n",
        "\n",
        "    fog, axes = plt.subplots(1,3 ,figsize = (18,6))\n",
        "    titles = ['Ground Truth' , 'Model Prediction' , f'Error Map({true_map != pred_map}) incorrect']\n",
        "    for ax , img ,title in zip(axes , [gt_img , pred_img , err_img] , titles):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "        ax.text(0.98 , 0.2 , 'shriansh.sahu' ,ha = 'right' ,va = 'bottom' ,transform = ax.transAxes , fontsize = 12, color ='white', alpha = 0.7)\n",
        "\n",
        "    map_path = os.path.join(run_folder , \"prediction_maps.png\")\n",
        "    plt.savefig(map_path)\n",
        "    plt.close()\n",
        "    wandb.log({\"prediction_analysis\" : wandb.Image(map_path)})\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def plot_architecture_results( results , x_values, x_label , run_folder):\n",
        "\n",
        "    losses = [r['loss'] for r in results]\n",
        "    accuracies = [r['accuracy'] for r in results]\n",
        "\n",
        "    fig,(ax1 , ax2) = plt.subplots(1 ,2 ,figsize = (15,6))\n",
        "\n",
        "    ax1.plot(x_values , losses , marker = 'o' , linestyle = '--')\n",
        "    ax1.set_title(f'Final Loss vs Model{x_label.capitalize()}')\n",
        "    ax1.set_xlabel(f'Model {x_label.capitalize()}')\n",
        "    ax1.set_ylabel('Final Loss')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(x_values , accuracies , marker = 'o' ,linestyle ='--' ,color = 'g')\n",
        "    ax2.set_title(f'Final Accuracy vs Model{x_label.capitalize()}')\n",
        "    ax2.set_xlabel(f'Model {x_label.capitalize()}')\n",
        "    ax2.set_ylabel('Final Accurcay(%)')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    add_username_watermark()\n",
        "    plt.sca(ax1)\n",
        "    add_username_watermark()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(run_folder , f\"architecture_{x_label}_analysis.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_model(model,dataset,config):\n",
        "\n",
        "    run_name = f\"{config['name']}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    run_folder = os.path.join('runs' ,run_name)\n",
        "    os.makedirs(run_folder, exist_ok = True)\n",
        "    wandb.init(project = config['project_name'] ,name = run_name , config = config , reinit = True)\n",
        "\n",
        "    start_time = time.time()\n",
        "    epoch_losses, samples_seen_log, samples_seen = [],[],0\n",
        "    pbar = tqdm(range(config['epochs']) , desc = f\"Training {config['name']}\", leave = False)\n",
        "    for epoch in pbar:\n",
        "        data , epoch_loss = dataset.get_shuffled_data(), 0.0\n",
        "        for i ,(coords , label) in enumerate(data):\n",
        "            loss,_ = model.train(np.array([coords]) ,np.array([[label]]))\n",
        "            epoch_loss += loss\n",
        "            if (i+1) % config['grad_accumulation_steps'] == 0:\n",
        "                model.update(config['learning_rate'])\n",
        "\n",
        "        samples_seen += len(data)\n",
        "        model.update(config['learning_rate'])\n",
        "        avg_loss = epoch_loss / len(data)\n",
        "        epoch_losses.append(avg_loss)\n",
        "        samples_seen_log.append(samples_seen)\n",
        "        pbar.set_description(f\"Epoch{epoch+1} | Loss : {avg_loss: .6f}\")\n",
        "        wandb.log({\"epoch\": epoch , \"loss\" : avg_loss , \"samples_seen\": samples_seen})\n",
        "        if epoch >= config['patience'] and avg_loss >= ( 1 - config['relative_loss_threshold']) * epoch_losses[epoch-config['patience']]:\n",
        "            print(f\"Early Stopping at epoch {epoch+1}.\")\n",
        "            break\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(samples_seen_log , epoch_losses)\n",
        "    plt.title(\"Loss vs Samples Seen\")\n",
        "    plt.xlabel(\"Samples Seen\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    add_username_watermark()\n",
        "    loss_plot_path = os.path.join(run_folder, \"loss_vs_samples.png\")\n",
        "    plt.savefig(loss_plot_path)\n",
        "    plt.close()\n",
        "    wandb.log({\"loss_plot\" : wandb.Image(loss_plot_path)})\n",
        "\n",
        "    final_accuracy = evaluate_and_visualize(model ,dataset ,run_folder)\n",
        "    wandb.finish()\n",
        "\n",
        "    return {'name' : run_name , 'loss' : epoch_losses[-1] , 'accuracy': final_accuracy , 'time' : time.time()  - start_time , 'samples_seen' : samples_seen}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "        os.makedirs(\"runs\" , exist_ok = True)\n",
        "\n",
        "        print(\"=\" *60)\n",
        "        print(\"1.3 RUNNING SANITY CHECKS\")\n",
        "        print(\"=\" *60)\n",
        "\n",
        "        xor_tester = XORProblem()\n",
        "        grad_checker = GradientChecker()\n",
        "        model_to_check = xor_tester.run_test([Linear(2, 8 ,Tanh) , Linear(8,1,Sigmoid)])\n",
        "        grad_checker.check(model_to_check , xor_tester.X[1:2] , xor_tester.y[1:2])\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60); print(\"TRAINING ON BORDER IMAGE\"); print(\"=\"*60)\n",
        "        border_dataset = ImageBorderDataset('/content/border.png')\n",
        "        base_config = {'epochs': 50, 'grad_accumulation_steps': 32, 'patience': 5, 'relative_loss_threshold': 0.01, 'learning_rate': 0.01}\n",
        "        configurations = [\n",
        "        { 'name': 'Shallow_Wide_ReLU', 'layers': [Linear(2, 128, ReLU), Linear(128, 1, Sigmoid)], 'lr': 0.01 },\n",
        "        { 'name': 'Deep_Narrow_Tanh', 'layers': [Linear(2, 32, Tanh), Linear(32, 32, Tanh), Linear(32, 1, Sigmoid)], 'lr': 0.01 },\n",
        "        ]\n",
        "        for arch_config in configurations:\n",
        "          train_params = {\n",
        "              'project_name': \"Traine Model on Border Dataset\", 'epochs': 150,\n",
        "              'grad_accumulation_steps': 32, 'patience': 10,\n",
        "              'relative_loss_threshold': 0.01, 'loss_function': 'bce'\n",
        "              }\n",
        "          full_config = {**arch_config, **train_params, 'learning_rate': arch_config['lr']}\n",
        "          border_model = Model(layers=full_config['layers'], loss_function=full_config['loss_function'])\n",
        "          train_model(border_model, border_dataset, full_config)\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80); print(\"1.4.3: ARCHITECTURE EXPERIMENTS\"); print(\"=\"*80)\n",
        "        # Varying Depth\n",
        "        print(\"\\n--- Experiment: Varying Depth (Width fixed at 64) ---\")\n",
        "        depths = [2, 3, 4, 5]; depth_results = []\n",
        "        for d in depths:\n",
        "            layers = [Linear(2, 64, ReLU)] + [Linear(64, 64, ReLU) for _ in range(d-2)] + [Linear(64, 1, Sigmoid)]\n",
        "            config = {**base_config, 'project_name': 'border-arch-depth', 'name': f\"Depth_{d}-Width_64\"}\n",
        "            depth_results.append(train_model(Model(layers, 'bce'), border_dataset, config))\n",
        "        plot_architecture_results(depth_results, depths, \"depth\", \"runs\")\n",
        "\n",
        "        # Varying Width\n",
        "        print(\"\\n--- Experiment: Varying Width (Depth fixed at 4) ---\")\n",
        "        widths = [16, 32, 64, 128]; width_results = []\n",
        "        for w in widths:\n",
        "            layers = [Linear(2, w, ReLU)] + [Linear(w, w, ReLU) for _ in range(2)] + [Linear(w, 1, Sigmoid)]\n",
        "            config = {**base_config, 'project_name': 'border-arch-width', 'name': f\"Depth_4-Width_{w}\"}\n",
        "            width_results.append(train_model(Model(layers, 'bce'), border_dataset, config))\n",
        "        plot_architecture_results(width_results, widths, \"width\", \"runs\")\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80); print(\"ðŸš€ 1.4.4: HYPERPARAMETER EXPERIMENTS\"); print(\"=\"*80)\n",
        "        hparam_configs = [\n",
        "            {'name': 'HighLR', 'learning_rate': 0.05, 'grad_accumulation_steps': 32},\n",
        "            {'name': 'LowLR', 'learning_rate': 0.005, 'grad_accumulation_steps': 32},\n",
        "            {'name': 'LargeBatch', 'learning_rate': 0.01, 'grad_accumulation_steps': 128},\n",
        "            {'name': 'SmallBatch', 'learning_rate': 0.01, 'grad_accumulation_steps': 8},\n",
        "            ]\n",
        "        hparam_results = []\n",
        "        for h_config in hparam_configs:\n",
        "            model_layers = [Linear(2, 64, ReLU), Linear(64, 64, ReLU), Linear(64, 1, Sigmoid)]\n",
        "            config = {**base_config, 'project_name': 'border-hparams', **h_config}\n",
        "            hparam_results.append(train_model(Model(model_layers, 'bce'), border_dataset, config))\n",
        "\n",
        "        print(\"\\n--- Hyperparameter Experimentation Summary ---\")\n",
        "        print(f\"{'Run Name':<25} | {'Accuracy (%)':<15} | {'Final Loss':<15} | {'Time (s)':<15} | {'Samples Seen':<15}\")\n",
        "        print(\"-\" * 90)\n",
        "        for res in hparam_results:\n",
        "            print(f\"{res['name']:<25} | {res['accuracy']:<15.2f} | {res['loss']:<15.4f} | {res['time']:<15.2f} | {res['samples_seen']:<15}\")\n",
        "\n",
        "        print(\"\\n All experiments completed.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
