{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvoRof3bX0ok"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "\n",
        "class Identity:\n",
        "    def forward(self,z) : return z\n",
        "    def backward(self,z): return np.ones(z.shape)\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self,z) : return 1/(1 + np.exp(-np.clip(z, -500 , 500)))\n",
        "    def backward(self,z):\n",
        "        s = self.forward(z)\n",
        "        return s * (1-s)\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self,z) : return np.tanh(z)\n",
        "    def backward(self,z): return 1 - np.tanh(z) ** 2\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self,z) : return np.maximum(0,z)\n",
        "    def backward(self,z): return (z >0).astype(float)\n",
        "\n",
        "class MSE:\n",
        "    def forward(self, y_true , y_pred):\n",
        "        return np.mean((y_pred - y_true)**2)\n",
        "\n",
        "    def backward(self, y_true , y_pred):\n",
        "        return 2*(y_pred - y_true) / y_true.size\n",
        "\n",
        "class BCE:\n",
        "    def forward(self, y_true , y_pred):\n",
        "        epsilon = 1e-5\n",
        "        y_pred = np.clip(y_pred , epsilon , 1-epsilon)\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    def backward(self, y_true , y_pred):\n",
        "       epsilon = 1e-5\n",
        "       y_pred = np.clip(y_pred , epsilon , 1-epsilon)\n",
        "       return ((1 - y_true) / ( 1 - y_pred) - y_true / y_pred) / y_true.size\n",
        "\n",
        "\n",
        "class Linear:\n",
        "\n",
        "    def __init__(self ,input_width  , output_width , activation ):\n",
        "        limit = np.sqrt(6 / (input_width + output_width))\n",
        "        self.weights = np.random.uniform(-limit , limit, ( input_width, output_width))\n",
        "        self.biases = np.zeros((1 , output_width))\n",
        "        self.activation = activation()\n",
        "        self.input , self.z , self.a = None, None, None\n",
        "        self.grad_weights_cumulative = np.zeros_like(self.weights)\n",
        "        self.grad_biases_cumulative = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input , self.z = input_data , np.dot(input_data , self.weights) + self.biases\n",
        "        self.a = self.activation.forward(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward( self , upstream_gradient):\n",
        "        d_z = upstream_gradient * self.activation.backward(self.z)\n",
        "        self.grad_weights_cumulative += np.dot(self.input.T , d_z)\n",
        "        self.grad_biases_cumulative += np.sum(d_z , axis = 0 , keepdims= True)\n",
        "        return np.dot(d_z , self.weights.T)\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, layers , loss_function):\n",
        "        self.layers , self.loss_fn = layers, BCE() if loss_function.lower() == 'bce' else MSE()\n",
        "\n",
        "    def forward(self, x):\n",
        "            for layer in self.layers : x = layer.forward(x)\n",
        "            return x\n",
        "\n",
        "    def backward(self, y_true , y_pred):\n",
        "            grad = self.loss_fn.backward(y_true , y_pred)\n",
        "            for layer in reversed(self.layers): grad = layer.backward(grad)\n",
        "\n",
        "    def train(self,x,y):\n",
        "            y_pred = self.forward(x)\n",
        "            loss = self.loss_fn.forward(y , y_pred)\n",
        "            self.backward(y , y_pred)\n",
        "            return loss , y_pred\n",
        "\n",
        "    def calculate_loss(self , x , y):\n",
        "            y_pred = self.forward(x)\n",
        "            loss = self.loss_fn.forward(y , y_pred)\n",
        "            return loss\n",
        "\n",
        "    def zero_grad(self):\n",
        "            for layer in self.layers:\n",
        "                layer.grad_weights_cumulative.fill(0)\n",
        "                layer.grad_biases_cumulative.fill(0)\n",
        "\n",
        "    def update(self, learning_rate):\n",
        "            for layer in self.layers:\n",
        "                layer.weights -= learning_rate * layer.grad_weights_cumulative\n",
        "                layer.biases -= learning_rate * layer.grad_biases_cumulative\n",
        "\n",
        "    def clip_gradient(self,threshold):\n",
        "\n",
        "      all_grads = [self.layers[i].grad_weights_cumulative.ravel() for i in range(len(self.layers))]\n",
        "      all_grads += [self.layers[i].grad_biases_cumulative for i in range(len(self.layers))]\n",
        "      all_grads_flat = np.concatenate(all_grads)\n",
        "\n",
        "      norm = np.linalg.norm(all_grads_flat)\n",
        "      if norm > threshold:\n",
        "        clip_factor= threshold / norm\n",
        "\n",
        "        for layer in self.layers:\n",
        "          layer.grad_weights_cumulative *= clip_factor\n",
        "          layer.grad_biases_cumulative *= clip_factor\n",
        "\n",
        "    def predict(self,x):\n",
        "            return self.forward(x)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(l.weights.size + l.biases.size for l in self.layers)\n",
        "\n",
        "class ImageBorderDataset:\n",
        "\n",
        "    def __init__(self, image_path):\n",
        "        self.pixels,self.width ,self.height = self._load_and_process_image(image_path)\n",
        "\n",
        "    def _load_and_process_image(self , image_path):\n",
        "        try: image = Image.open(image_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            arr = np.zeros((50,50,3), dtype=np.uint8)\n",
        "            arr[:25,:,:] = [255 , 165 ,0]\n",
        "            arr[25:,:,:] = [138 ,43 ,226]\n",
        "            image = Image.fromarray(arr)\n",
        "\n",
        "        img_array = np.array(image)\n",
        "        height ,width,_ = img_array.shape\n",
        "        data = [(((x / (width-1) ,y / (height -1))) , 1 if all(img_array[y,x][:2] < [150,50]) and img_array[y,x][2] > 100 else 0) for y in range(height) for x in range(width)]\n",
        "        return data, width , height\n",
        "\n",
        "    def get_shuffled_data(self):\n",
        "        shuffled  = self.pixels[:]\n",
        "        random.shuffle(shuffled)\n",
        "        return shuffled\n",
        "\n",
        "\n",
        "def evaluate(model, dataset):\n",
        "\n",
        "    ORANGE , PURPLE ,RED = [255,165,0],[138,43,226],[255,0,0]\n",
        "    coords = np.array([p[0] for p in dataset.pixels])\n",
        "    true_labels = np.array([p[1] for p in dataset.pixels])\n",
        "    pred_labels = (model.predict(coords) > 0.5).astype(int).flatten()\n",
        "    accuracy  = np.mean(pred_labels == true_labels) * 100\n",
        "    return accuracy, None\n",
        "\n",
        "\n",
        "def train_model(model,dataset,config):\n",
        "\n",
        "    run_name = config['name']\n",
        "    pbar = tqdm(range(config['epochs']) , desc = f\"Training {run_name}\", leave = True)\n",
        "\n",
        "    samples_seen = 0\n",
        "    final_accuracy = 0\n",
        "\n",
        "    for epoch in pbar:\n",
        "        data , epoch_loss = dataset.get_shuffled_data(), 0.0\n",
        "        for i ,(coords , label) in enumerate(data):\n",
        "            loss,_ = model.train(np.array([coords]) ,np.array([[label]]))\n",
        "            epoch_loss += loss\n",
        "            if (i+1) % config['grad_accumulation_steps'] == 0:\n",
        "                model.update(config['learning_rate'])\n",
        "\n",
        "        samples_seen += len(data)\n",
        "        model.update(config['learning_rate'])\n",
        "\n",
        "        current_accuracy, _ = evaluate(model, dataset)\n",
        "        pbar.set_description(f\"Epoch {epoch+1} / {config['epochs']} | Loss : {epoch_loss/len(data):.4f} | Accuracy : {current_accuracy:.2f}%\")\n",
        "\n",
        "        if config.get('stop_at_target', False) and current_accuracy >= config['target_accuracy']:\n",
        "            print(f\"Reached target accuracy of {config['target_accuracy']} at epoch {epoch+1}\")\n",
        "            final_accuracy = current_accuracy\n",
        "            break\n",
        "        final_accuracy = current_accuracy\n",
        "    return{'sample_seen' : samples_seen , 'accuracy' : final_accuracy}\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    border_dataset = ImageBorderDataset('/content/border.png')\n",
        "    TARGET_ACCURACY = 91.0\n",
        "\n",
        "\n",
        "    print(f\"1.5 Goal 1: Minimize Model Size to reach {TARGET_ACCURACY}% Accuracy\")\n",
        "\n",
        "\n",
        "    min_param_layers = [Linear(2, 48, Tanh)] + \\\n",
        "                       [Linear(48, 48, Tanh) for _ in range(3)] + \\\n",
        "                       [Linear(48, 1, Sigmoid)]\n",
        "\n",
        "    min_param_model = Model(layers=min_param_layers, loss_function='bce')\n",
        "\n",
        "\n",
        "    config_goal1 = {\n",
        "        'name': \"Goal1_Final_4x48_Tanh\",\n",
        "        'epochs': 150,\n",
        "        'learning_rate': 0.01,\n",
        "        'grad_accumulation_steps': 64,\n",
        "        'patience': 10,\n",
        "        'relative_loss_threshold': 0.01,\n",
        "        'target_accuracy': TARGET_ACCURACY,\n",
        "        'stop_at_target': True\n",
        "    }\n",
        "\n",
        "    print(f\"Testing Corrected Architecture: 4 hidden layers, width 48 with Tanh.\")\n",
        "    print(f\"Total Parameters: {min_param_model.count_parameters()}\")\n",
        "\n",
        "\n",
        "    result_goal1 = train_model(min_param_model, border_dataset, config_goal1)\n",
        "\n",
        "    print(\"\\n--- Goal 1 Summary ---\")\n",
        "    print(f\"Architecture Parameters: {min_param_model.count_parameters()}\")\n",
        "    print(f\"Final Accuracy: {result_goal1['accuracy']:.2f}%\")\n",
        "    if result_goal1['accuracy'] >= TARGET_ACCURACY:\n",
        "        print(\"Success: Target accuracy achieved.\")\n",
        "    else:\n",
        "        print(\"Failure: Target accuracy not met.\")\n",
        "\n",
        "\n",
        "    print(f\" 1.5 Goal 2: Minimize Training Samples to reach {TARGET_ACCURACY}% Accuracy\")\n",
        "\n",
        "    min_samples_model = Model(layers=[Linear(2, 48, Tanh)] + \\\n",
        "                                     [Linear(48, 48, Tanh) for _ in range(3)] + \\\n",
        "                                     [Linear(48, 1, Sigmoid)],\n",
        "                              loss_function='bce')\n",
        "\n",
        "    config_goal2 = {\n",
        "        'name': \"Goal2_Final_GradClip\",\n",
        "        'epochs': 60,\n",
        "        'learning_rate': 0.02,\n",
        "        'grad_accumulation_steps': 128,\n",
        "        'patience': 10,\n",
        "        'relative_loss_threshold': 0.01,\n",
        "        'target_accuracy': TARGET_ACCURACY,\n",
        "        'grad_clip_threshold': 1.0\n",
        "    }\n",
        "\n",
        "    print(f\"Testing with LR={config_goal2['learning_rate']} and Gradient Clipping (Threshold={config_goal2['grad_clip_threshold']})\")\n",
        "\n",
        "    pbar = tqdm(range(config_goal2['epochs']), desc=f\"Training {config_goal2['name']}\", leave=True)\n",
        "    samples_seen = 0\n",
        "    final_accuracy = 0\n",
        "\n",
        "    for epoch in pbar:\n",
        "        data, epoch_loss = border_dataset.get_shuffled_data(), 0.0\n",
        "        for i, (coords, label) in enumerate(data):\n",
        "            loss, _ = min_samples_model.train(np.array([coords]), np.array([[label]]))\n",
        "            epoch_loss += loss\n",
        "            if (i+1) % config_goal2['grad_accumulation_steps'] == 0:\n",
        "\n",
        "                min_samples_model.clip_gradient(threshold=config_goal2['grad_clip_threshold'])\n",
        "                min_samples_model.update(config_goal2['learning_rate'])\n",
        "\n",
        "\n",
        "        min_samples_model.clip_gradient(threshold=config_goal2['grad_clip_threshold'])\n",
        "        min_samples_model.update(config_goal2['learning_rate'])\n",
        "        samples_seen += len(data)\n",
        "\n",
        "        current_accuracy, _ = evaluate(min_samples_model, border_dataset)\n",
        "        pbar.set_description(f\"Loss: {epoch_loss/len(data):.4f} | Acc: {current_accuracy:.2f}%\")\n",
        "\n",
        "        if current_accuracy >= TARGET_ACCURACY:\n",
        "            print(f\"\\n Target {TARGET_ACCURACY}% accuracy reached!\")\n",
        "            final_accuracy = current_accuracy\n",
        "            break\n",
        "        final_accuracy = current_accuracy\n",
        "\n",
        "    print(\"\\n--- Goal 2 Summary ---\")\n",
        "    print(f\"Final Accuracy: {final_accuracy:.2f}%\")\n",
        "    print(f\"Minimum Samples to Converge: {samples_seen}\")\n",
        "    if final_accuracy >= TARGET_ACCURACY:\n",
        "        print(\"Success: Target accuracy achieved with minimal samples.\")\n",
        "    else:\n",
        "        print(\"Failure: Target accuracy not met within the given epochs.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "1.5: FINAL CHALLENGE (Final & Robust Attempt)\n",
        "\n",
        "--- Goal 1: Minimize Model Size to reach 91.0% ---\n",
        "Testing Final Architecture: 4 hidden layers, width 48 with Tanh. Parameters: 7249\n",
        "Epoch 121/150 | Loss: 0.2506 | Acc: 91.04%:  80%|████████  | 120/150 [01:20<00:20,  1.50it/s]\n",
        "Target 91.0% accuracy reached!\n",
        "\n",
        "--- Goal 1 Summary ---\n",
        "Architecture Parameters: 7249\n",
        "Final Accuracy: 91.04%\n",
        "Success: Target accuracy achieved.\n",
        "\n",
        "--- Goal 2: Minimize Samples to reach 91.0% ---\n",
        "Testing with LR=0.02 and Gradient Clipping (Threshold=1.0)\n",
        "Loss: 0.3486 | Acc: 83.24%: 100%|██████████| 60/60 [00:39<00:00,  1.54it/s]\n",
        "--- Goal 2 Summary ---\n",
        "Final Accuracy: 83.24%\n",
        "Minimum Samples to Converge: 150000\n",
        "Failure: Target accuracy not met within the given epochs."
      ],
      "metadata": {
        "id": "Vm0CY8G7X8KF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}